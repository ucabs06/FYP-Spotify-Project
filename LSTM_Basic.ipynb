{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mini data\n",
    "log =  pd.read_csv('log_mini.csv', low_memory=False)\n",
    "track_f =  pd.read_csv('tf_mini.csv', low_memory=False)\n",
    "\n",
    "# all track featrue data \n",
    "# track_f1= pd.read_csv('tf_000000000000.csv', low_memory=False)\n",
    "# track_f2 = pd.read_csv('tf_000000000001.csv', low_memory=False)\n",
    "# tf_all = pd.concat([track_f1,track_f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackfeatures_preprocessing(track_f):\n",
    "    # build track_id dictionary \n",
    "    track_id_dict = pd.unique(track_f.track_id)\n",
    "    track_id_dict = pd.Series(np.arange(len(track_id_dict)), track_id_dict).to_dict()\n",
    "    \n",
    "    # convert major/minor  to boolean\n",
    "    track_f['mode'] = track_f['mode'].apply(lambda x: True if 'major' in x else False)\n",
    "    \n",
    "    # standardize \n",
    "    scaler = StandardScaler()\n",
    "    track_f = scaler.fit_transform(track_f.iloc[:,1:])\n",
    "    \n",
    "\n",
    "    return track_f, track_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "track_f, track_id_dict = trackfeatures_preprocessing(track_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_preprocessing(log):\n",
    "    \n",
    "    # build session id dictionary \n",
    "    session_id_dict = pd.unique(log.session_id)\n",
    "    session_id_dict = pd.Series(np.arange(len(session_id_dict)), session_id_dict).to_dict()\n",
    "    \n",
    "    # replace track_id in session data \n",
    "    log.loc[:, \"track_id_clean\"] = np.vectorize(track_id_dict.get)(log.track_id_clean)\n",
    "    \n",
    "    #split date \n",
    "#     log['year'] =  pd.to_datetime(log['date'], format='%Y-%m-%d').dt.year \n",
    "#     log['month'] = pd.to_datetime(log['date'], format='%Y-%m-%d').dt.month\n",
    "#     log['day'] = pd.to_datetime(log['date'], format='%Y-%m-%d').dt.day\n",
    "#     log = log.drop(['date'],axis = 1)\n",
    "\n",
    "    # Min-max scaler for numerical data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     numer_columns = ['hist_user_behavior_n_seekfwd','hist_user_behavior_n_seekback']\n",
    "#     log[numer_columns] = pd.DataFrame(scaler.fit_transform(log[numer_columns]))\n",
    "    \n",
    "    # One-hot encoding for categorical data \n",
    "#     categ_columns = ['skip_1','skip_2','skip_3','not_skipped','context_switch','no_pause_before_play','short_pause_before_play','long_pause_before_play',\n",
    "#                'hist_user_behavior_is_shuffle','hour_of_day','day','month','year', 'premium','context_type','hist_user_behavior_reason_start',\n",
    "#                'hist_user_behavior_reason_end']\n",
    "#     log = pd.get_dummies(log, columns = categ_columns)\n",
    "    log['skip_2'] = log['skip_2']*1\n",
    "    \n",
    "    \n",
    "    return log,session_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "log,session_id_dict = session_preprocessing(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_f_tensor = torch.FloatTensor(track_f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input_prepare(data):\n",
    "    position = np.array(data['session_position'])\n",
    "    length = np.array(data['session_length'])\n",
    "    total_session = []\n",
    "    start = 0\n",
    "    for i in range(len(position)):\n",
    "        if position[i] == length[i]:\n",
    "            end = i\n",
    "            total_session.append(torch.LongTensor(data['track_id_clean'][start:end].values))\n",
    "            start = i+1\n",
    "    Train_X = total_session[0:8000]\n",
    "    Test_X = total_session[8000:]\n",
    "    return Train_X,Test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X,Test_X = Input_prepare(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prepare(data):\n",
    "    # split session \n",
    "    session_split_idx = np.empty(shape=(0,), dtype=np.int32)\n",
    "    index_start_from = 0\n",
    "    session_split_idx = np.hstack((session_split_idx, np.insert(np.cumsum(np.unique(data.session_id.values, return_counts=True)[1])[:-1], 0, 0) + index_start_from))   \n",
    "    session_split_idx = session_split_idx[1:]\n",
    "    \n",
    "    Y = []\n",
    "    for i in session_split_idx:\n",
    "        Y.append(data['skip_2'][i-1] )\n",
    "    Train_Y = Y[0:8000]\n",
    "    Test_Y = Y[8000:]\n",
    "    return Train_Y,Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y, Test_Y = label_prepare(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self,weight_matrix_of_track, embedding_dim, hidden_dim, num_layers, num_directions ,batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions \n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_dim = embedding_dim  \n",
    "        self.embedding_track= nn.Embedding.from_pretrained(weight_matrix_of_track)     # track embedding layer\n",
    "        self.lstm=nn.LSTM(self.embedding_dim,self.hidden_dim,batch_first=True)\n",
    "        self.ln1 = nn.Linear(hidden_dim, 128)\n",
    "        self.ln2 = nn.Linear(128,1)\n",
    "        \n",
    "#     def init_hidden(self):\n",
    "#         return(Variable(torch.zeros(1,self.batch_size,self.hidden_dim)),Variable(torch.zeros(1,self.batch_size,self.hidden_dim)))\n",
    "    def forward(self,session): \n",
    "        embeds_session=self.embedding_track(session)    # session embedding \n",
    "#         packed_input = pack_padded_sequence(embeds_session, session_lengths)     # pack     \n",
    "#         lstm_out, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "#         padded_out,recovered_len = pad_packed_sequence(lstm_out, batch_first=True)                  # unpack \n",
    "#         tag_space = self.linear(padded_out.view(len(session), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space)   \n",
    "\n",
    "        embeds_session = embeds_session.view(self.batch_size,-1,self.embedding_dim)\n",
    "        #embeds_session = embeds_session.view(-1,self.batch_size, self.embedding_dim)\n",
    "        #embeds_session = embeds_session.view(len(session),self.batch_size, -1)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.batch_size,self.num_layers*self. num_directions, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.batch_size,self.num_layers*self. num_directions, self.hidden_dim))\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds_session, (h0, c0))\n",
    "        \n",
    "        out = F.sigmoid(self.ln1(lstm_out[:,-1,:]))\n",
    "        output = F.sigmoid(self.ln2(out))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy\n",
    "def accuracy(test_session,test_label,model):\n",
    "    total = len(test_label)\n",
    "    correct=0\n",
    "    for session,labels in zip(test_session,test_label):  \n",
    "        labels = torch.LongTensor([test_label])\n",
    "        _, predictions = torch.max(model(session), dim = 1)             \n",
    "        correct += (predictions == labels).sum()\n",
    "        return float(correct)/total  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LSTMClassifier(weight_matrix_of_track =track_f_tensor , embedding_dim = 29 , hidden_dim = 256 , num_layers = 1,\n",
    "                     num_directions =1 , batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1000/8000], Loss: 0.6204, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [2000/8000], Loss: 0.5402, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [3000/8000], Loss: 0.5296, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [4000/8000], Loss: 0.2788, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [5000/8000], Loss: 1.1514, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [6000/8000], Loss: 0.3446, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [7000/8000], Loss: 0.3245, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [8000/8000], Loss: 1.3772, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [1000/8000], Loss: 0.5791, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [2000/8000], Loss: 0.3858, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [3000/8000], Loss: 0.3262, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [4000/8000], Loss: 0.2454, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [5000/8000], Loss: 0.6710, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [6000/8000], Loss: 0.5099, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [7000/8000], Loss: 0.6625, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [8000/8000], Loss: 1.0907, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [1000/8000], Loss: 0.4325, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [2000/8000], Loss: 0.4136, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [3000/8000], Loss: 0.3844, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [4000/8000], Loss: 0.3218, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [5000/8000], Loss: 0.6024, Train_Acc: 0.5974\n",
      "Epoch: [3/10], Step: [6000/8000], Loss: 0.6348, Train_Acc: 0.5974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-65c85b5eb8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model=LSTMClassifier(weight_matrix_of_track =track_f_tensor , embedding_dim = 29 , hidden_dim = 256 , num_layers = 1,\n",
    "                     num_directions =1 , batch_size = 1)\n",
    "criteria =nn.BCELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "epoch_accuracies = []\n",
    "best_accuracy=0\n",
    "No_epochs = 10 \n",
    "\n",
    "for epoch in range(No_epochs):\n",
    "    running_loss = 0\n",
    "    i=0\n",
    "    for session,labels in zip(Train_X,Train_Y):\n",
    "        i=i+1\n",
    "        labels = torch.FloatTensor([labels])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(session)     \n",
    "        # labels=labels.unsqueeze(dim=0)    \n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = criteria(outputs,labels)\n",
    "        running_loss += loss.data[0] \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%1000==0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Train_Acc: %.4f'\n",
    "                     % (epoch+1, 10, i+1, len(Train_X), \n",
    "                        loss.data, accuracy(Train_X,Train_Y,model) ))\n",
    "    epoch_accuracies.append(accuracy(Test_X,Test_Y,model))\n",
    "\n",
    "#         if epoch_accuracies[-1] > best_accuracy:\n",
    "#     best_accuracy = epoch_accuracies[-1]\n",
    "#     best_epoch = epoch\n",
    "#     if(epoch>=5 and epoch_accuracies[-1]<=epoch_accuracies[-5]):\n",
    "#     break        \n",
    "#   # TODO: implement early stopping here      \n",
    "#   print(best_accuracy, best_epoch)\n",
    "\n",
    "# Q batch size ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
