{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import pathlib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "log =  pd.read_csv('log_mini.csv', low_memory=False)\n",
    "track_f =  pd.read_csv('tf_mini.csv', low_memory=False)\n",
    "\n",
    "# all track featrue data \n",
    "# track_f1= pd.read_csv('tf_000000000000.csv', low_memory=False)\n",
    "# track_f2 = pd.read_csv('tf_000000000001.csv', low_memory=False)\n",
    "# tf_all = pd.concat([track_f1,track_f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackfeatures_preprocessing(track_f):\n",
    "    # build track_id dictionary \n",
    "    track_id_dict = pd.unique(track_f.track_id)\n",
    "    track_id_dict = pd.Series(np.arange(len(track_id_dict)), track_id_dict).to_dict()\n",
    "    \n",
    "    # convert major/minor  to boolean\n",
    "    track_f['mode'] = track_f['mode'].apply(lambda x: True if 'major' in x else False)\n",
    "    \n",
    "    # standardize \n",
    "    scaler = StandardScaler()\n",
    "    track_f = scaler.fit_transform(track_f.iloc[:,1:])\n",
    "    \n",
    "    return track_f, track_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype bool, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "track_f, track_id_dict = trackfeatures_preprocessing(track_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_preprocessing(log):\n",
    "    \n",
    "    # build session id dictionary \n",
    "    session_id_dict = pd.unique(log.session_id)\n",
    "    session_id_dict = pd.Series(np.arange(len(session_id_dict)), session_id_dict).to_dict()\n",
    "    \n",
    "    # replace track_id in session data \n",
    "    log.loc[:, \"track_id_clean\"] = np.vectorize(track_id_dict.get)(log.track_id_clean)\n",
    "    \n",
    "    #split date \n",
    "#     log['year'] =  pd.to_datetime(log['date'], format='%Y-%m-%d').dt.year \n",
    "#     log['month'] = pd.to_datetime(log['date'], format='%Y-%m-%d').dt.month\n",
    "#     log['day'] = pd.to_datetime(log['date'], format='%Y-%m-%d').dt.day\n",
    "#     log = log.drop(['date'],axis = 1)\n",
    "\n",
    "    # Min-max scaler for numerical data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     numer_columns = ['hist_user_behavior_n_seekfwd','hist_user_behavior_n_seekback']\n",
    "#     log[numer_columns] = pd.DataFrame(scaler.fit_transform(log[numer_columns]))\n",
    "    \n",
    "    # One-hot encoding for categorical data \n",
    "#     categ_columns = ['skip_1','skip_2','skip_3','not_skipped','context_switch','no_pause_before_play','short_pause_before_play','long_pause_before_play',\n",
    "#                'hist_user_behavior_is_shuffle','hour_of_day','day','month','year', 'premium','context_type','hist_user_behavior_reason_start',\n",
    "#                'hist_user_behavior_reason_end']\n",
    "#     log = pd.get_dummies(log, columns = categ_columns)\n",
    "    log['skip_2'] = log['skip_2']*1\n",
    "    return log,session_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "log,session_id_dict = session_preprocessing(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_f_tensor = torch.FloatTensor(track_f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input_prepare(data):\n",
    "    position = np.array(data['session_position'])\n",
    "    length = np.array(data['session_length'])\n",
    "    total_session = []\n",
    "    start = 0\n",
    "    for i in range(len(position)):\n",
    "        if position[i] == length[i]:\n",
    "            end = i\n",
    "            total_session.append(torch.LongTensor(data['track_id_clean'][start:end].values))\n",
    "            start = i+1\n",
    "    Train_X = total_session[0:8000]\n",
    "    Test_X = total_session[8000:]\n",
    "    return Train_X,Test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X,Test_X = Input_prepare(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prepare(data):\n",
    "    # split session \n",
    "    session_split_idx = np.empty(shape=(0,), dtype=np.int32)\n",
    "    index_start_from = 0\n",
    "    session_split_idx = np.hstack((session_split_idx, np.insert(np.cumsum(np.unique(data.session_id.values, return_counts=True)[1])[:-1], 0, 0) + index_start_from))   \n",
    "    session_split_idx = session_split_idx[1:]\n",
    "    \n",
    "    Y = []\n",
    "    for i in session_split_idx:\n",
    "        Y.append(data['skip_2'][i-1] )\n",
    "    Train_Y = Y[0:8000]\n",
    "    Test_Y = Y[8000:]\n",
    "    return Train_Y,Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y, Test_Y = label_prepare(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_prepare(data):\n",
    "    # split session \n",
    "    session_split_idx = np.empty(shape=(0,), dtype=np.int32)\n",
    "    index_start_from = 0\n",
    "    session_split_idx = np.hstack((session_split_idx, np.insert(np.cumsum(np.unique(data.session_id.values, return_counts=True)[1])[:-1], 0, 0) + index_start_from))   \n",
    "    session_split_idx = session_split_idx[1:]\n",
    "    \n",
    "    candidate_session = []\n",
    "    for i in session_split_idx:\n",
    "        candidate_session.append((data['track_id_clean'][i-1]))\n",
    "    candidate_session = torch.LongTensor(candidate_session) \n",
    "    Train_X_candi = candidate_session[0:8000]\n",
    "    Test_X_candi = candidate_session[8000:]\n",
    "    return Train_X_candi,Test_X_candi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_candi,Test_X_candi = candidate_prepare(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_1 = log[0:10007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position_1 =position [0:10007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session_split_idx_1 = session_split_idx[0:591]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combine track feature and get new track embedding  dimension = 58 \n",
    "# for i in range(len(position_1)):\n",
    "#     for j in session_split_idx_1:\n",
    "#         if position_1[i] < j:\n",
    "#             tf[log_1['track_id_clean'][i]] = np.append(track_f[log_1['track_id_clean'][i]], track_f[log_1['track_id_clean'][j-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('track_feature_log',(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf =np.loadtxt('track_feature_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self,weight_matrix_of_track, embedding_dim, hidden_dim, num_layers, num_directions ,batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions \n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_dim = embedding_dim  \n",
    "        self.embedding_track= nn.Embedding.from_pretrained(weight_matrix_of_track)     # track embedding layer\n",
    "        self.lstm=nn.LSTM(self.embedding_dim,self.hidden_dim,batch_first=True)\n",
    "        self.ln1 = nn.Linear(hidden_dim, 128)\n",
    "        self.ln2 = nn.Linear(128,1)\n",
    "        \n",
    "#     def init_hidden(self):\n",
    "#         return(Variable(torch.zeros(1,self.batch_size,self.hidden_dim)),Variable(torch.zeros(1,self.batch_size,self.hidden_dim)))\n",
    "    def forward(self,session,candidate_session): \n",
    "        embeds_session = self.embedding_track(session)    # session embedding \n",
    "        embeds_candidate_session = self.embedding_track(candidate_session)\n",
    "        \n",
    "        # empand to current embeds_session size \n",
    "        embeds_candidate_session = embeds_candidate_session.expand( len(session), 29)\n",
    "        # merge two embeddings into one \n",
    "        merged = torch.cat([embeds_session,embeds_candidate_session],1)\n",
    "        merged_session = merged.view(self.batch_size,-1,self.embedding_dim)\n",
    "        h0 = Variable(torch.zeros(self.batch_size,self.num_layers*self. num_directions, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.batch_size,self.num_layers*self. num_directions, self.hidden_dim))\n",
    "        \n",
    "        lstm_out, _ = self.lstm( merged_session, (h0, c0))\n",
    "        \n",
    "        out = F.sigmoid(self.ln1(lstm_out[:,-1,:]))\n",
    "        output = F.sigmoid(self.ln2(out))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy\n",
    "def accuracy(test_session, Test_X_candi, test_label,model):\n",
    "    total = len(test_label)\n",
    "    correct=0\n",
    "    for session,candidate_session, labels in zip(test_session, Test_X_candi,  test_label):  \n",
    "        labels = torch.LongTensor([test_label])\n",
    "        _, predictions = torch.max(model(session,candidate_session), dim = 1)             \n",
    "        correct += (predictions == labels).sum()\n",
    "        return float(correct)/total  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sue/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/Users/sue/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1000/8000], Loss: 0.9987, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [2000/8000], Loss: 0.6399, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [3000/8000], Loss: 0.3831, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [4000/8000], Loss: 0.3820, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [5000/8000], Loss: 0.5796, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [6000/8000], Loss: 0.5629, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [7000/8000], Loss: 0.5128, Train_Acc: 0.5974\n",
      "Epoch: [1/10], Step: [8000/8000], Loss: 0.7330, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [1000/8000], Loss: 0.5105, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [2000/8000], Loss: 0.5564, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [3000/8000], Loss: 0.4448, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [4000/8000], Loss: 0.3940, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [5000/8000], Loss: 0.5310, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [6000/8000], Loss: 0.5139, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [7000/8000], Loss: 0.5493, Train_Acc: 0.5974\n",
      "Epoch: [2/10], Step: [8000/8000], Loss: 0.8956, Train_Acc: 0.5974\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(weight_matrix_of_track =track_f_tensor , embedding_dim = 58 , hidden_dim = 256 , num_layers = 1,\n",
    "                     num_directions =1 , batch_size = 1)\n",
    "criteria =nn.BCELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
    "epoch_accuracies = []\n",
    "best_accuracy=0\n",
    "No_epochs = 2\n",
    "\n",
    "for epoch in range(No_epochs):\n",
    "    running_loss = 0\n",
    "    i=0\n",
    "    for session,candidate_session, labels in zip(Train_X,Train_X_candi,Train_Y):\n",
    "        i=i+1\n",
    "        labels = torch.FloatTensor([labels])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(session,candidate_session)     \n",
    "        # labels=labels.unsqueeze(dim=0)    \n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = criteria(outputs,labels)\n",
    "        running_loss += loss.data[0] \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%1000==0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Train_Acc: %.4f'\n",
    "                     % (epoch+1, 10, i+1, len(Train_X), \n",
    "                        loss.data, accuracy(Train_X,Train_X_candi,Train_Y,model) ))\n",
    "    epoch_accuracies.append(accuracy(Test_X,Test_X_candi, Test_Y,model))\n",
    "\n",
    "#         if epoch_accuracies[-1] > best_accuracy:\n",
    "#     best_accuracy = epoch_accuracies[-1]\n",
    "#     best_epoch = epoch\n",
    "#     if(epoch>=5 and epoch_accuracies[-1]<=epoch_accuracies[-5]):\n",
    "#     break        \n",
    "#   # TODO: implement early stopping here      \n",
    "#   print(best_accuracy, best_epoch)\n",
    "\n",
    "# Q batch size ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
